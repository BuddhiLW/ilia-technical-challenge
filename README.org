#+title: Notes on the challenge

The answer to the problems 1 to 4 can be read in the respective tag a section is dedicated to. I.e., Following [[https://gitlab.com/devprodexp/nameko-devexp/-/blob/main/README-DevEnv.md][README-DevEnv]] (1), etc.


TOC:
- [[*Following README-DevEnv (1)][Following README-DevEnv (1)]]
- [[*Running the application (1)][Running the application (1)]]
- [[*Features (2, 3)][Features (2, 3)]]
- [[*Execute performance test (4)][Execute performance test (4)]]
- [[*Performance Tests - Second try (4)][Performance Tests - Second try (4)]]


* DONE Following [[https://gitlab.com/devprodexp/nameko-devexp/-/blob/main/README-DevEnv.md][README-DevEnv]] (1)
DEADLINE: <2023-10-27 Fri> SCHEDULED: <2023-10-26 Thu>
** DONE Installation
DEADLINE: <2023-10-26 Thu> SCHEDULED: <2023-10-26 Thu>
Prerequisite:
- =conda=

You can install the dependencies and create the environment in =apt-derived= systems, like Ubuntu, running:
#+begin_src bash
bash basic-setup-and-run.bash
#+end_src

It will create the =conda= environment =nameko-devex=

#+begin_quote
To activate this environment, use

$ conda activate nameko-devex

To deactivate an active environment, use

$ conda deactivate
#+end_quote

Finally, the script will
- Create backings;
- Start the services;
- Run a simple test to see if everything run smoothly.

** DONE Running the services
*** DONE Initializing =script= after installation
#+begin_src shell
bash startup.sh
#+end_src

*** DONE Development

#+begin_src bash
./dev_run_backingsvcs.sh && ./dev_run.sh gateway.service orders.service products.service &
#+end_src

*** DONE Simple test
See if the project is running as supposed:

#+begin_src shell
./test/nex-smoketest.sh local
#+end_srC
** DONE Expected output
#+ATTR_HTML: :width 800px
[[file:doc-stuff/img/expected-output.png]]

* DONE Running the application (1)
DEADLINE: <2023-10-27 Fri> SCHEDULED: <2023-10-27 Fri>
** DONE Run in debugger mode
#+ATTR_HTML: :width 800px
[[file:doc-stuff/img/running-with-debug.png]]
** DONE Performance Test (via Taurus BlazeMeter locally)
#+ATTR_HTML: :width 1000px
[[file:doc-stuff/img/performance-test.png]]

* DONE Features (2, 3)
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-27 Fri>
** DONE Product Service (2)
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-28 Sat>
    #+begin_quote
    2. Enhance product service
    - Delete product rpc call
    - Wire into smoketest-sh
    - Wire into perf-test
    - Wire unit-test for this method
    #+end_quote
         
*** DONE =Delete-product= rpc call (2)
In [[file:products/products/service.py][products/products/service.py]], p.38.

The idea is to receive an =id= and search it the db, then delete it.

In =gateway/gateway/service.py= - Line 77, we add a instruction sequence of steps to be followed, upon deletion call (e.g., =curl -X "DELETE"=).

It will call the method =delete=, from =products_rpc=.

If successful, the response will be to return the id from the product deleted.

#+begin_src python
@http(
    "DELETE", "/products/<string:product_id>",
    expected_exceptions=ProductNotFound
)
def delete_product(self, request, product_id):
    """Gets product by `product_id` and delete it
    """

    # ------- Delete the product
    self.products_rpc.delete(product_id)

    # Respond with the product_id -- means it was a successeful a call
    return Response(
        # ProductSchema().dumps({'id': product_id}).data,
        # mimetype='application/json',
        status=204
    )
#+end_src

We write the =StorageWrapper= method for the client,
#+begin_src python
def delete(self, product_id):
    product = self.client.hgetall(self._format_key(product_id))
    self.client.delete(product_id)

    if not product:
        raise NotFound('Product ID {} does not exist'.format(product_id))
    else:
        return self._from_hash(product)
#+end_src

Also, we increment the =delete= method, in the server, located in =products/products/service.py= - Line 33,
#+begin_src python
@rpc
def delete(self, product_id):
    self.storage.delete(product_id)
#+end_src

*** DONE Wire =delete-product= into [[file:test/nex-smoketest.sh][nex-smoketest.sh]] (2)
The command to smoketest is:
#+begin_src bash
./test/nex-smoketest.sh local
#+end_src

And thus we add these lines to the bash script, in order to cover deleting a product.
#+begin_src bash
# Test: Delete Product
echo "=== Deleting product ==="
RESPONSE=$(curl -s -X "DELETE" "${STD_APP_URL}/products/the_odyssey")

if [ "${RESPONSE}" = "" ]; then
    echo "Successeful deletion"
else
    echo "Error: ${RESPONSE}"
fi
echo
#+end_src

Returns an empty body, but with =204= header.

*** DONE Wire into =perf-test= (2)
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>
#+begin_src bash
./test/nex-bzt.sh local
#+end_src

In order to insert the test case, we shall modify the =yml= file digested, in order to run the *performance tests*.

Therefore, inserting the following test-case in the file =test/nex-bzt.yml= - Line 111, does the job:

#+begin_src yaml
    # 5. Delete Product
    - url: /products/${product_id}
      label: product-delete
      think-time: uniform(0s, 0s)
      method: DELETE

      assert:
        - contains:
            - 204
            subject: http-code
            not: false
            extract-jsonpath:
              product_key: $.id
              default: NOT_FOUND

              - if: '"${order_id}" == "NOT_FOUND"'
                then:
                  - action: continue
#+end_src

And, we can see that the deletion has uniform performance, as expected =O(0)=. It bumps up, by =user-quantity=, but remains constant in that level, for each =user-quantity= (*in pink*).

link: https://a.blazemeter.com/app/?public-token=vRqk9enPpD9w6S2t9n3IxN8FJ2O8P5HzAWHfGEfZsajlartbRZ#reports/r-ext-653ea3ec2d196637735670/summary

#+ATTR_HTML: :width 1000px
[[file:doc-stuff/img/deletion-performance.png]]

#+ATTR_HTML: :width 1000px
[[file:doc-stuff/img/deletion-performance2.png]]

*** DONE Wire unit-test for this method (2)
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>

Add =TestDeleteProduct= class in =gateway/test/interface/test_service.py= - Line 296

#+begin_src python
class TestDeleteProduct(object):
    def test_can_delete_product(self, gateway_service, web_session):
        gateway_service.products_rpc.delete.return_value = ""
        response = web_session.delete("/products/the_odyssey")
        assert response.status_code == 204
        assert gateway_service.products_rpc.delete.call_args_list == [
            call("the_odyssey")
        ]

    def test_product_not_found(self, gateway_service, web_session):
        gateway_service.products_rpc.delete.side_effect = ProductNotFound("missing")

        # call the gateway service to get order #1
        response = web_session.delete("/products/foo")
        assert response.status_code == 404
        payload = response.json()
        assert payload["error"] == "PRODUCT_NOT_FOUND"
        assert payload["message"] == "missing"
#+end_src

The corresponding execution passes *100%* of the time, with a deprecation warning that has nothing to do with the new =feature-implementation= and =wiring=.

#+ATTR_HTML: :width 1000px
[[file:doc-stuff/img/product-deletion-unit-test.png]]

** DONE Orders Service (3)
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>

#+begin_quote
3. Enhance order service
  - List orders rpc call
  - Wire into smoketest.sh
  - Wire into perf-test
  - Wire unit-test for this method
#+end_quote

*** DONE =List-orders= rpc call
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>
**** DONE Gateway service implementation
Create the server response to =GET= call without =id= specification.

In, =gateway/gateway/service.py=,

#+begin_src python
# ------ Get all orders
@http("GET", "/orders/", expected_exceptions=EmptyOrders)
def get_orders(self, request):
    """Gets the order details for all orders."""

    orders = self._get_orders()
    return Response(
        GetOrderSchema().dumps(orders, many=True).data, mimetype="application/json"
    )

def _get_orders(self):
    # Retrieve all orders data, from the orders_rpc service.
    return self.orders_rpc.get_orders()
#+end_src

**** DONE Implement =EmptyOrders= exception
Create =EmptyOrders= exception, in =gateway/gateway/exceptions.py=:
#+begin_src python
@remote_error("orders.exceptions.NotFound")
class EmptyOrders(Exception):
    """
    When no order has been found
    """

    pass
#+end_src

**** DONE List orders rpc call
In =orders/orders/service=, we specify the =orders_rpc= method =get_orders= that corresponds to *list-orders*

#+begin_src python
# feature: Get all orders
@rpc
def get_orders(self):
    orders = self.db.query(Order).all()

    if len(orders) == 0:
        raise NotFound("No orders found")
    else:
        return OrderSchema().dump(orders, many=True).data
#+end_src

*** DONE Wire into smoketest.sh

#+begin_src bash
# Test: Get All Orders
echo "=== Gerring All Orders ==="
curl -s "${STD_APP_URL}/orders/" | jq .
#+end_src

#+ATTR_HTML: :width 300px
[[file:doc-stuff/img/list-all-smoketest.png]]

*** DONE Wire into perf-test
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>

Add =List All Orders= case, with the =list-orders= label, in =next-bzt.yml=,
#+begin_src yml
    # 6. List All Orders
    - url: /orders
      label: list-orders
      think-time: uniform(0s, 0s)
      method: GET

      assert:
      - contains:
        - 200
        subject: http-code
        not: false
      extract-jsonpath:
        default: NOT_FOUND
#+end_src

Link: 
https://a.blazemeter.com/app/?public-token=6kcJWXU5rUh81DVuWHK3PCwHVlffoJN5iV98vIUdJWRr5FVtXb#/accounts/-1/workspaces/-1/projects/-1/sessions/r-ext-653ee23e634bf313148657/summary

#+ATTR_HTML: :width 1200px
[[file:doc-stuff/img/list-perf-test.png]]

The purple line gives us almost a constant speed, over time and user numbers, because it should only be porportional to the time o =access-memory= time. Which, thus, makes sense.

*** DONE Wire unit-test for this method
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>

In =orders/test/interface/test_service.py=, we can add these assertions to test if the =list-orders= feature is working as supposed to.

Let's create 10 orders, first. Then, test for two things:
- Does the =list-all= method will return a list of length 10?
- Are the orders actually in the db?

#+begin_src python
@pytest.mark.usefixtures("db_session")
def test_list_all_orders(orders_rpc, db_session):
    order_details = [
        {"product_id": "the_odyssey", "price": 99.99, "quantity": 1},
        {"product_id": "the_enigma", "price": 5.99, "quantity": 8},
    ]

    for _ in range(10):
        orders_rpc.create_order(OrderDetailSchema(many=True).dump(order_details).data)

    response = orders_rpc.get_orders()
    assert len(response) == 10
    assert len(response) == len(db_session.query(Order).all())
#+end_src

#+ATTR_HTML: :width 500px
[[file:doc-stuff/img/orders-unit-test.png]]
* DONE Performance Tests (4)

#+begin_quote
- Question 1: Why is performance degrading as the test runs longer?
- Question 2: How do you fix it?
- Fix the performance issue.
#+end_quote

** Q1
#+begin_quote
Why is performance degrading as the test runs longer?
#+end_quote

The issue with performance may be due to a lack of caching, in the =Orders= service, in contrast with the use of =Redis= (local-caching), in the =Products= service.

Image caption: Difference in performance =Orders= vs =Products= service calls.
#+ATTR_HTML: :width 1200px
[[file:doc-stuff/img/order-vs-product.png]]

Due to limited resources of a server in terms of memory and CPU, repeting a common call can become a burden on the server.

** Q2
#+begin_quote
How do you fix it?
#+end_quote

Implement some sort of caching as a wrapper/decorator. For example, using =TTL= (interal/local) or =Redis= (external).

** Q3
#+begin_quote
Fix the performance issue.
#+end_quote

In order to debug and test further =Hypothesis 2= (connection overload), as well as the =Caching Hypothesis=, we explicitly turn on =echo=True= and fine-tune =pool_recycle= in seconds.

With not much difference in performance, we also turn on =expire_on_commit= and =auto_flush=.

This all done in =orders/orders/service.py=

#+begin_src python
class OrdersService:
    name = "orders"

    DB_URIS = "DB_URIS"

    db_orders = Database(DeclarativeBase)
    event_dispatcher = EventDispatcher()
    engine = create_engine(
        config.get(DB_URIS)["orders:Base"], echo=True, pool_recycle=240
    )
    method_session = scoped_session(
        sessionmaker(
            engine,
            expire_on_commit=True,
            autoflush=True,
        )
    )
#+end_src

*** TTL decorators (caching)
Now, implementing the decorators in =orders/caching.py=. First, It was tried =TTLCache=; one decorator per corresponding method.

#+begin_src python
from nameko import config
from cachetools import TTLCache
from functools import wraps
import hashlib  # We'll use hashlib to create cache keys
import redis
import json

# Create an in-memory cache with a max size and time-to-live (TTL)
cache_co = TTLCache(maxsize=500, ttl=600)  # cache for co: Create Order
cache_get = TTLCache(maxsize=500, ttl=600)  # cache for get: Get Order
cache_list = TTLCache(maxsize=500, ttl=600)  # cache for list: List Orders


def cache_get_order(func):
    @wraps(func)
    def wrapper(self, order_id):
        # Generate a cache key based on the method name and parameters
        cache_key = f"{func.__name__}:{order_id}"

        # Check if the result is in the cache
        cached_result = cache_get.get(cache_key)
        if cached_result is not None:
            return cached_result

        # If not in the cache, execute the method and store the result
        result = func(self, order_id)
        cache_get[cache_key] = result

        return result

    return wrapper


def cache_list_orders(func):
    @wraps(func)
    def wrapper(self):
        # Generate a cache key based on the method name and parameters
        cache_key = func.__name__

        # Check if the result is in the cache
        cached_result = cache_list.get(cache_key)
        if cached_result is not None:
            return cached_result

        # If not in the cache, execute the method and store the result
        result = func(self)
        cache_list[cache_key] = result

        return result

    return wrapper


def cache_create_order(func):
    @wraps(func)
    def wrapper(self, order_details):
        # Generate a cache key based on the method name and parameters
        cache_key = hashlib.md5(str(order_details).encode()).hexdigest()

        # Check if the result is in the cache
        cached_result = cache_co.get(cache_key)
        if cached_result is not None:
            return cached_result

        # If not in the cache, execute the method and store the result
        result = func(self, order_details)
        cache_co[cache_key] = result

        return result

    return wrapper
#+end_src

Finally, we add the decorators in =orders/service.py=, like this:

#+begin_src python
@rpc
@cache_list_orders
def list_orders(self):
    with self.method_session() as session:
        orders = session.query(Order).all()
        data = OrderSchema().dump(orders, many=True).data

        session.close()
        return data
#+end_src

*** Result of =TTS= caching
This gave a satisfactorily solved the performance issue for =list-orders= (listing all orders).

Before adding the decorators:
#+ATTR_HTML: :width 500px
[[file:doc-stuff/img/performance-test-4step.png]]


After adding the decorators (=TTL=):
#+ATTR_HTML: :width 1000px
[[file:doc-stuff/img/list-order-diff-pos.png]]

We note =list-orders= went from an =average-rate= of 0.229 to 0.004!


Also, the =hits/s= (+38%), =avg. response time= (-23%) and =bandwidth= (22x lower) all improved. Still, =order-get= and =order-create= didn't flinch.

Graphically, we see a smoother curve:
#+ATTR_HTML: :width 1200px
[[file:doc-stuff/img/diff-overall1.png]]
*** =Redis= decorators (caching)

Trying to solve the same way =order-get= and =orders-create=, but with =Redis= because =TTS= didn't cut it, we write these decorators:

#+begin_src python
# Create a Redis connection
REDIS_URI_KEY = "REDIS_URI"
redis_client = redis.StrictRedis(decode_responses=True).from_url(
    config.get(REDIS_URI_KEY)
)


def cache_get_with_redis(func):
    @wraps(func)
    def wrapper(self, order_id):
        # Generate a cache key based on the method name and parameters
        cache_key = f"{func.__name__}:{order_id}"

        # Check if the result is in the Redis cache
        cached_result = redis_client.get(cache_key)
        if cached_result is not None:
            # If cached result exists, deserialize and return it
            return json.loads(cached_result)

        # If not in the cache, execute the method and store the result in Redis
        result = func(self, order_id)

        # Cache the result as a JSON string with an expiration time (e.g., 300 seconds)
        redis_client.setex(cache_key, 300, json.dumps(result))
        return result

    return wrapper


def cache_result_with_redis(cache_prefix, ttl):
    def decorator(func):
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            # Serialize the arguments to use as a cache key
            cache_key = (
                cache_prefix + hashlib.md5(json.dumps(args).encode()).hexdigest()
            )

            # Check if the result is in the Redis cache
            cached_result = redis_client.get(cache_key)
            if cached_result is not None:
                # If cached result exists, deserialize and return it
                return json.loads(cached_result)

            # If not in the cache, execute the method and store the result in Redis
            result = func(self, *args, **kwargs)
            # Cache the result as a JSON string with the specified TTL
            redis_client.setex(cache_key, ttl, json.dumps(result))

            return result

        return wrapper

    return decorator
#+end_src

And plug them in get and create orders, like so:

#+begin_src python
@rpc
@cache_get_with_redis
def get_order(self, order_id):
    with self.method_session() as session:
        order = session.query(Order).get(order_id)

        if not order:
            raise NotFound("Order with id {} not found".format(order_id))

        data = OrderSchema().dump(order).data

        session.close()
        return data


@rpc
@cache_result_with_redis("create_orders", 600)
def create_order(self, order_details):
   with self.method_session() as session:
       order = Order(
           order_details=[
               OrderDetail(
                   product_id=order_detail["product_id"],
                   price=order_detail["price"],
                   quantity=order_detail["quantity"],
               )
               for order_detail in order_details
           ]
       )

       session.add(order)
       session.commit()

       order = OrderSchema().dump(order).data

       self.event_dispatcher(
           "order_created",
           {
               "order": order,
           },
       )

       session.close()
       return order
#+end_src

*** Result caching with =Redis= and =TTL= and differences

After =Redis caching=:
https://a.blazemeter.com/app/?public-token=32UEZ8InnnmZx7rudCQPDRhLZjfu2SHPCnOVWQwWm6puocLOwz#/accounts/-1/workspaces/-1/projects/-1/sessions/r-ext-6541a5fd545f4521666578/summary/summary

#+ATTR_HTML: :width 500px
[[file:doc-stuff/img/redis-caching-result.png]]

I think that by empirically fine-tuning, it's possible to remove the bell-shaped tail of the response time, also.

The main notable difference between using =Redis= and =TTL= to try to solve this performance issue was the shape of the response-time. While =TTL= maintained the linear increase over use, =Redis= gave us a bell-curve.

*** Result of changing connection parameters

Changing connection parameters won't change overall shape of the =response-time=, but will have the effect of a sinusoidal convolution.

The magnitude of the problem, though, will remain the same, in average.

#+ATTR_HTML: :width 1200px
[[file:doc-stuff/img/parameter-wiggle.png]]
*** Final thought
I was unable to either track or implement the debugging of the =order-get= and =order-create= methods.

The most promising hypothesis is, although, the =caching=. Because of the effect it gave to =list-orders= and also it's one of the main differences in implementation in the =Orders= service and the =Products= service.
* DONE Checking any possible issues with the DB (4)
** DONE Check that =postgres= is running in =docker=
#+begin_src bash
docker ps
#+end_src

#+begin_quote
CONTAINER ID    IMAGE    COMMAND                CREATED         STATUS           PORTS                                          NAMES

c521ba004734   postgres "docker-entrypoint.s…" 47 minutes ago   Up 47 minutes   0.0.0.0:5432->5432/tcp, :::5432->5432/tcp   devPostgres
#+end_quote

** DONE Check that the container doesn't present errors internally
#+begin_src bash
docker logs devPostgres
#+end_src

We can see from the =logs= that *"... database system is ready to accept connections"*,

#+begin_quote
PostgreSQL init process complete; ready for start up.

2023-11-01 15:14:30.112 UTC [1] LOG:  starting PostgreSQL 16.0 (Debian 16.0-1.pgdg120+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
2023-11-01 15:14:30.113 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
2023-11-01 15:14:30.113 UTC [1] LOG:  listening on IPv6 address "::", port 5432
2023-11-01 15:14:30.118 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
2023-11-01 15:14:30.127 UTC [61] LOG:  database system was shut down at 2023-11-01 15:14:30 UTC
2023-11-01 15:14:30.132 UTC [1] LOG:  database system is ready to accept connections
2023-11-01 15:19:30.137 UTC [59] LOG:  checkpoint starting: time
2023-11-01 15:19:34.254 UTC [59] LOG:  checkpoint complete: wrote 44 buffers (0.3%); 0 WAL file(s) added, 0 removed, 0 recycled; write=4.110 s, sync=0.003 s, total=4.118 s; sync files=11, longest=0.002 s, average=0.001 s; distance=260 kB, estimate=260 kB; lsn=0/152BEA8, redo lsn=0/152BE70
#+end_quote

** DONE Check connection to the database is possible

Since the =config.yml= presents the following line:
#+begin_src yml
DB_URIS:
    "orders:Base": ${POSTGRES_URI:"postgres://postgres:postgres@localhost:5432/orders"}
#+end_src

After running:
#+begin_src bash
./dev_run.sh gateway.service orders.service products.service
#+end_src

It will execute:
#+begin_src bash
# create database orders locally
python -c """import psycopg2 as db;p='postgres';con=db.connect(dbname=p,host='localhost',user=p,password=p);
con.autocommit=True;con.cursor().execute('CREATE DATABASE orders')""" 2> /dev/null
#+end_src

We can test the connection like so:
#+begin_src bash
docker exec -it devPostgres psql -U postgres -d orders
#+end_src

#+begin_quote
psql (16.0 (Debian 16.0-1.pgdg120+1))
Type "help" for help.

orders=#
#+end_quote

So, it's working.

# *** Error

# And we get the following *error*:

# #+begin_quote
# psql: error: connection to server on socket "/var/run/postgresql/.s.PGSQL.5432" failed: FATAL:  database "orders" does not exist
# #+end_quote

# *** Solution

# Using the command =createdb=, to create the =orders= database, for the user =postgres=.

# #+begin_src bash
# docker exec -it devPostgres createdb -U postgres orders
# #+end_src

# And, we can verify now the connection is working:

# #+begin_src bash
# docker exec -it devPostgres psql -U postgres -d orders
# #+end_src

# #+begin_quote
# psql (16.0 (Debian 16.0-1.pgdg120+1))
# Type "help" for help.

# orders=#
# #+end_quote

# *** See if performance issue went away

# We see that it didn't:
# https://a.blazemeter.com/app/?public-token=D2WbR9e8XFCP1T49InTxcweqIDbajKRzK1SrvDlN8dOkp6e7XE#reports/r-ext-65427b5953ee3058060260/summary

# Let's continue debugging
** DONE Make sure the data is going through the database
#+begin_src bash
sudo docker exec -it devPostgres psql -U postgres
#+end_src

#+begin_src bash
\c orders
SELECT * FROM "orders";
#+end_src

#+begin_quote
  id  |         created_at         |         updated_at
------+----------------------------+---------------------------
    1 | 2023-11-01 17:43:51.223211 | 2023-11-01 17:43:51.223218
    2 | 2023-11-01 17:43:51.309809 | 2023-11-01 17:43:51.309815
    3 | 2023-11-01 17:43:51.357695 | 2023-11-01 17:43:51.357702
    4 | 2023-11-01 17:43:51.404509 | 2023-11-01 17:43:51.404516
    5 | 2023-11-01 17:43:51.455504 | 2023-11-01 17:43:51.455511
    6 | 2023-11-01 17:43:51.499745 | 2023-11-01 17:43:51.499751
    7 | 2023-11-01 17:43:51.552222 | 2023-11-01 17:43:51.552229
    8 | 2023-11-01 17:43:51.595596 | 2023-11-01 17:43:51.595602

(...)
#+end_quote

We see the data is going through alright.
