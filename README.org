#+title: Notes on the challenge

The answer to the problems 1 to 4 can be read in the respective tag a section is dedicated to. I.e., Following [[https://gitlab.com/devprodexp/nameko-devexp/-/blob/main/README-DevEnv.md][README-DevEnv]] (1), etc.

* DONE Following [[https://gitlab.com/devprodexp/nameko-devexp/-/blob/main/README-DevEnv.md][README-DevEnv]] (1)
DEADLINE: <2023-10-27 Fri> SCHEDULED: <2023-10-26 Thu>
** DONE Installation
DEADLINE: <2023-10-26 Thu> SCHEDULED: <2023-10-26 Thu>
Prerequisite:
- =conda=

You can install the dependencies and create the environment in =apt-derived= systems, like Ubuntu, running:
#+begin_src bash
bash basic-setup-and-run.bash
#+end_src

It will create the =conda= environment =nameko-devex=

#+begin_quote
To activate this environment, use

$ conda activate nameko-devex

To deactivate an active environment, use

$ conda deactivate
#+end_quote

Finally, the script will
- Create backings;
- Start the services;
- Run a simple test to see if everything run smoothly.

** DONE Running the services
*** DONE Initializing =script= after installation
#+begin_src shell
bash startup.sh
#+end_src

*** DONE Development

#+begin_src bash
./dev_run_backingsvcs.sh && ./dev_run.sh gateway.service orders.service products.service &
#+end_src

*** DONE Simple test
See if the project is running as supposed:

#+begin_src shell
./test/nex-smoketeSt.sh local
#+end_src
** DONE Expected output
#+ATTR_HTML: :width 800px
[[file:doc-stuff/img/expected-output.png]]

* DONE Running the application (1)
DEADLINE: <2023-10-27 Fri> SCHEDULED: <2023-10-27 Fri>
** DONE Run in debugger mode
#+ATTR_HTML: :width 800px
[[file:doc-stuff/img/running-with-debug.png]]
** DONE Performance Test (via Taurus BlazeMeter locally)
#+ATTR_HTML: :width 1000px
[[file:doc-stuff/img/performance-test.png]]

* DONE Features (2, 3)
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-27 Fri>
** DONE Product Service (2)
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-28 Sat>
    #+begin_quote
    2. Enhance product service
    - Delete product rpc call
    - Wire into smoketest-sh
    - Wire into perf-test
    - Wire unit-test for this method
    #+end_quote
         
*** DONE =Delete-product= rpc call (2)
In [[file:products/products/service.py][products/products/service.py]], p.38.

The idea is to receive an =id= and search it the db, then delete it.

In =gateway/gateway/service.py= - Line 77, we add a instruction sequence of steps to be followed, upon deletion call (e.g., =curl -X "DELETE"=).

It will call the method =delete=, from =products_rpc=.

If successful, the response will be to return the id from the product deleted.

#+begin_src python
@http(
    "DELETE", "/products/<string:product_id>",
    expected_exceptions=ProductNotFound
)
def delete_product(self, request, product_id):
    """Gets product by `product_id` and delete it
    """

    # ------- Delete the product
    self.products_rpc.delete(product_id)

    # Respond with the product_id -- means it was a successeful a call
    return Response(
        # ProductSchema().dumps({'id': product_id}).data,
        # mimetype='application/json',
        status=204
    )
#+end_src

We write the =StorageWrapper= method for the client,
#+begin_src python
def delete(self, product_id):
    product = self.client.hgetall(self._format_key(product_id))
    self.client.delete(product_id)

    if not product:
        raise NotFound('Product ID {} does not exist'.format(product_id))
    else:
        return self._from_hash(product)
#+end_src

Also, we increment the =delete= method, in the server, located in =products/products/service.py= - Line 33,
#+begin_src python
@rpc
def delete(self, product_id):
    self.storage.delete(product_id)
#+end_src

*** DONE Wire =delete-product= into [[file:test/nex-smoketest.sh][nex-smoketest.sh]] (2)
The command to smoketest is:
#+begin_src bash
./test/nex-smoketest.sh local
#+end_src

And thus we add these lines to the bash script, in order to cover deleting a product.
#+begin_src bash
# Test: Delete Product
echo "=== Deleting product ==="
RESPONSE=$(curl -s -X "DELETE" "${STD_APP_URL}/products/the_odyssey")

if [ "${RESPONSE}" = "" ]; then
    echo "Successeful deletion"
else
    echo "Error: ${RESPONSE}"
fi
echo
#+end_src

Returns an empty body, but with =204= header.

*** DONE Wire into =perf-test= (2)
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>
#+begin_src bash
./test/nex-bzt.sh local
#+end_src

In order to insert the test case, we shall modify the =yml= file digested, in order to run the *performance tests*.

Therefore, inserting the following test-case in the file =test/nex-bzt.yml= - Line 111, does the job:

#+begin_src yaml
    # 5. Delete Product
    - url: /products/${product_id}
      label: product-delete
      think-time: uniform(0s, 0s)
      method: DELETE

      assert:
        - contains:
            - 204
            subject: http-code
            not: false
            extract-jsonpath:
              product_key: $.id
              default: NOT_FOUND

              - if: '"${order_id}" == "NOT_FOUND"'
                then:
                  - action: continue
#+end_src

And, we can see that the deletion has uniform performance, as expected =O(0)=. It bumps up, by =user-quantity=, but remains constant in that level, for each =user-quantity= (*in pink*).

link: https://a.blazemeter.com/app/?public-token=vRqk9enPpD9w6S2t9n3IxN8FJ2O8P5HzAWHfGEfZsajlartbRZ#reports/r-ext-653ea3ec2d196637735670/summary

#+ATTR_HTML: :width 1000px
[[file:doc-stuff/img/deletion-performance.png]]

#+ATTR_HTML: :width 1000px
[[file:doc-stuff/img/deletion-performance2.png]]

*** DONE Wire unit-test for this method (2)
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>

Add =TestDeleteProduct= class in =gateway/test/interface/test_service.py= - Line 296

#+begin_src python
class TestDeleteProduct(object):
    def test_can_delete_product(self, gateway_service, web_session):
        gateway_service.products_rpc.delete.return_value = ""
        response = web_session.delete("/products/the_odyssey")
        assert response.status_code == 204
        assert gateway_service.products_rpc.delete.call_args_list == [
            call("the_odyssey")
        ]

    def test_product_not_found(self, gateway_service, web_session):
        gateway_service.products_rpc.delete.side_effect = ProductNotFound("missing")

        # call the gateway service to get order #1
        response = web_session.delete("/products/foo")
        assert response.status_code == 404
        payload = response.json()
        assert payload["error"] == "PRODUCT_NOT_FOUND"
        assert payload["message"] == "missing"
#+end_src

The corresponding execution passes *100%* of the time, with a deprecation warning that has nothing to do with the new =feature-implementation= and =wiring=.

#+ATTR_HTML: :width 1000px
[[file:doc-stuff/img/product-deletion-unit-test.png]]

** DONE Orders Service (3)
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>

#+begin_quote
3. Enhance order service
  - List orders rpc call
  - Wire into smoketest.sh
  - Wire into perf-test
  - Wire unit-test for this method
#+end_quote

*** DONE =List-orders= rpc call
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>
**** DONE Gateway service implementation
Create the server response to =GET= call without =id= specification.

In, =gateway/gateway/service.py=,

#+begin_src python
# ------ Get all orders
@http("GET", "/orders/", expected_exceptions=EmptyOrders)
def get_orders(self, request):
    """Gets the order details for all orders."""

    orders = self._get_orders()
    return Response(
        GetOrderSchema().dumps(orders, many=True).data, mimetype="application/json"
    )

def _get_orders(self):
    # Retrieve all orders data, from the orders_rpc service.
    return self.orders_rpc.get_orders()
#+end_src

**** DONE Implement =EmptyOrders= exception
Create =EmptyOrders= exception, in =gateway/gateway/exceptions.py=:
#+begin_src python
@remote_error("orders.exceptions.NotFound")
class EmptyOrders(Exception):
    """
    When no order has been found
    """

    pass
#+end_src

**** DONE List orders rpc call
In =orders/orders/service=, we specify the =orders_rpc= method =get_orders= that corresponds to *list-orders*

#+begin_src python
# feature: Get all orders
@rpc
def get_orders(self):
    orders = self.db.query(Order).all()

    if len(orders) == 0:
        raise NotFound("No orders found")
    else:
        return OrderSchema().dump(orders, many=True).data
#+end_src

*** DONE Wire into smoketest.sh

#+begin_src bash
# Test: Get All Orders
echo "=== Gerring All Orders ==="
curl -s "${STD_APP_URL}/orders/" | jq .
#+end_src

#+ATTR_HTML: :width 300px
[[file:doc-stuff/img/list-all-smoketest.png]]

*** DONE Wire into perf-test
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>

Add =List All Orders= case, with the =list-orders= label, in =next-bzt.yml=,
#+begin_src yml
    # 6. List All Orders
    - url: /orders
      label: list-orders
      think-time: uniform(0s, 0s)
      method: GET

      assert:
      - contains:
        - 200
        subject: http-code
        not: false
      extract-jsonpath:
        default: NOT_FOUND
#+end_src

Link: 
https://a.blazemeter.com/app/?public-token=6kcJWXU5rUh81DVuWHK3PCwHVlffoJN5iV98vIUdJWRr5FVtXb#/accounts/-1/workspaces/-1/projects/-1/sessions/r-ext-653ee23e634bf313148657/summary

#+ATTR_HTML: :width 1200px
[[file:doc-stuff/img/list-perf-test.png]]

The purple line gives us almost a constant speed, over time and user numbers, because it should only be porportional to the time o =access-memory= time. Which, thus, makes sense.

*** DONE Wire unit-test for this method
DEADLINE: <2023-10-29 Sun> SCHEDULED: <2023-10-29 Sun>

In =orders/test/interface/test_service.py=, we can add these assertions to test if the =list-orders= feature is working as supposed to.

Let's create 10 orders, first. Then, test for two things:
- Does the =list-all= method will return a list of length 10?
- Are the orders actually in the db?

#+begin_src python
@pytest.mark.usefixtures("db_session")
def test_list_all_orders(orders_rpc, db_session):
    order_details = [
        {"product_id": "the_odyssey", "price": 99.99, "quantity": 1},
        {"product_id": "the_enigma", "price": 5.99, "quantity": 8},
    ]

    for _ in range(10):
        orders_rpc.create_order(OrderDetailSchema(many=True).dump(order_details).data)

    response = orders_rpc.get_orders()
    assert len(response) == 10
    assert len(response) == len(db_session.query(Order).all())
#+end_src

#+ATTR_HTML: :width 500px
[[file:doc-stuff/img/orders-unit-test.png]]
* DONE Execute performance test (4)
DEADLINE: <2023-10-31 Tue> SCHEDULED: <2023-10-29 Sun>
#+begin_quote
- Question 1: Why is performance degrading as the test runs longer?
- Question 2: How do you fix it?
- Fix the performance issue.
#+end_quote

Let's execute the performance test once again.

#+begin_src bash
./test/nex-bzt.sh local
#+end_src

#+RESULTS: https://a.blazemeter.com/app/?public-token=w8ReaRiTEV6quhuRCpJWszuikIqeTH4uQalaF0vSZgkEI1MxP6#reports/r-ext-653fb211b17ab737942637/summary

#+ATTR_HTML: :width 500px
[[file:doc-stuff/img/performance-test-4step.png]]

** DONE Why is performance degrading as the test runs longer? (Q1)
DEADLINE: <2023-10-31 Tue> SCHEDULED: <2023-10-29 Sun>

*** Hypothesis number one - Data Structures
Analyzing the =response-time= chart over time. In this test-scenario, =time= (*t*) is directly proportional to =quantity of orders= (*No*).

We see that the operations =order-get=, =order-create= and =list-orders= seem to linearly correlate to the =quantity of orders= (*No*).

That is to say that these *search* and *insertion* algorithms employed do not represent an optimal solution, since it's well-known that both operations can be =O(log(n)=, in a balanced /binary search tree/ (BST) - which can be a python dictionary.

#+ATTR_HTML: :width 1300px
[[file:doc-stuff/img/chart-execution-time-non-optimal.png]]
*** Hypothesis number two - DB-Connections overhead (*Chosen explanation*)

The session calls to connect to the database are actually accumulating over the calls. This increases, linearly-proportion to the number of calls, the overhead of the next call.

This hypothesis seems the most plausible, over the fact, and the easiest to modify.

Thus, the *solution* for fixing the performance will attack this hypothesis.

** DONE How do you fix it? (Q2)
DEADLINE: <2023-10-31 Tue> SCHEDULED: <2023-10-29 Sun>

*** Hypothetical Solution for hypothesis one
One solution would be to implement the same storage of data with a different *data structure*. In other words, one could solve this problem by representing the same data with a different /data structure/, so the *search* and *insertion* operations be =O(log(n))=.

Another solution would be to implement a *binary search* in the *list of objects*, instead of a linear search.

Currently, the aggregate of orders has a stored-representation of a *list of objects*.

*** Hypothetical Solution for hypothesis two (my solution)
Reading the https://docs.sqlalchemy.org/ documentation, one finds that calling the =self.db.query()= is deprecated.

And, one should opt for the following:

#+begin_src python
db = Database(DeclarativeBase)

def method(self, order_id):
    with self.db_orders.get_session() as session:
        # Operate on session here
#+end_src

This approach will diminish the number of =alive sessions=. And, therefore, diminish the call-overhead for any operation.

** DONE Fix the performance issue
DEADLINE: <2023-10-31 Tue> SCHEDULED: <2023-10-29 Sun>

In =orders/orders/service.py=, I changed the =db= instance-declaration from:

#+begin_src python
db = DatabaseSession(DeclarativeBase)
#+end_src

To:
#+begin_src python
db_orders = Database(DeclarativeBase)
#+end_src

Also, in every occurrence of:
#+begin_src python
self.db.query(Order)*
#+end_src

I wrapped the section dealing with the =db=, with instead:
#+begin_src python
with self.db_orders.get_session() as session:
    session.query(Order)*
#+end_src

*** Results
The final report can be found at: https://a.blazemeter.com/app/?public-token=OUr335F30q95J2p1NNPSeZl83TvufXg75DgE7OIKMjgTqrcKpf#reports/r-ext-6540212b57597553416121/summary

And, this is the side-to-side comparison of =get-order=, for example:

#+ATTR_HTML: :width 1200px
[[file:doc-stuff/img/execution-time-comparison-get-order.png]]

The before and after, from left to right. We see the performance is now much more attached, one-to-one with the number of =hits= / =requests= at a given time, instead of accumulating over executions.
